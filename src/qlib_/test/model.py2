import torch
import torch.nn as nn

class BidirectionalGRU3L128H_HybridPool_BN_L2_Residual_Adaptive(torch.nn.Module):
    def __init__(self, num_features, num_timesteps=50, hidden_size=128, num_layers=3, dropout_rate=0.2, l2_lambda=0.001):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout_rate = dropout_rate
        self.l2_lambda = l2_lambda  # Store L2 lambda for optimizer configuration

        # GRU layers (3 layers with bidirectional)
        self.gru1 = nn.GRU(num_features, hidden_size, num_layers=1, bidirectional=True, batch_first=True)
        self.gru2 = nn.GRU(hidden_size * 2, hidden_size, num_layers=1, bidirectional=True, batch_first=True)
        self.gru3 = nn.GRU(hidden_size * 2, hidden_size, num_layers=1, bidirectional=True, batch_first=True)

        # BatchNorm layers
        self.bn1 = nn.BatchNorm1d(hidden_size * 2)
        self.bn2 = nn.BatchNorm1d(hidden_size * 2)
        self.bn3 = nn.BatchNorm1d(hidden_size * 2)

        # Dropout and activation
        self.dropout = nn.Dropout(dropout_rate)
        self.activation = nn.LeakyReLU()

        # Residual projections with dimension matching
        self.residual_proj1 = nn.Linear(num_features, hidden_size * 2)  # Ensures input features map to GRU output dim
        self.residual_proj2 = nn.Linear(hidden_size * 2, hidden_size * 2)
        self.residual_proj3 = nn.Linear(hidden_size * 2, hidden_size * 2)

        # Hybrid pooling
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)

        # Final fully connected layer
        self.fc = nn.Linear(hidden_size * 4, 1)  # 2 pools * 2 (avg/max) * hidden_size*2

    def forward(self, x):
        # GRU Layer 1
        out, _ = self.gru1(x)
        residual = self.residual_proj1(x)
        out = out + residual[:, :out.size(1), :]  # Ensure same sequence length
        out = self.bn1(out.transpose(1, 2)).transpose(1, 2)
        out = self.activation(out)
        out = self.dropout(out)

        # GRU Layer 2
        out, _ = self.gru2(out)
        residual = self.residual_proj2(out)
        out = out + residual[:, :out.size(1), :]  # Ensure same sequence length
        out = self.bn2(out.transpose(1, 2)).transpose(1, 2)
        out = self.activation(out)
        out = self.dropout(out)

        # GRU Layer 3
        out, _ = self.gru3(out)
        residual = self.residual_proj3(out)
        out = out + residual[:, :out.size(1), :]  # Ensure same sequence length
        out = self.bn3(out.transpose(1, 2)).transpose(1, 2)
        out = self.activation(out)
        out = self.dropout(out)

        # Hybrid Pooling
        avg = self.avg_pool(out.transpose(1, 2)).squeeze(2)
        max = self.max_pool(out.transpose(1, 2)).squeeze(2)
        combined = torch.cat([avg, max], dim=1)

        # Final prediction
        return self.fc(combined)

model_cls = BidirectionalGRU3L128H_HybridPool_BN_L2_Residual_Adaptive