
import os
import pickle
from datetime import datetime, date
from pathlib import Path
import json
from typing import Tuple, Any

import yaml
import qlib
import qlib
from qlib.config import REG_CN, C
from qlib.utils import init_instance_by_config

from qlib.contrib.model.gbdt import LGBModel
from qlib.contrib.data.handler import Alpha158
from qlib.utils import init_instance_by_config, flatten_dict
from qlib.workflow import R
from qlib.workflow.record_temp import SignalRecord, PortAnaRecord, SigAnaRecord


def init_qlib(config_path='') -> tuple[Any, Path]:
    # 1. è¯»å–é…ç½®
    with open(config_path, "r") as f:
        cfg = yaml.safe_load(f)

    # 2. åˆå§‹åŒ– Qlib
    qlib_init = cfg.get("qlib_init", {})

    base_dir = qlib_init.get("exp_manager").get("kwargs").get('uri').replace('file:', '')
    base_dir_path = Path(base_dir)
    work_dir = base_dir_path.parent
    print(f'å·¥ä½œç›®å½•ï¼š{work_dir}')
    # print(work_dir)
    if not work_dir.exists():
        os.makedirs(work_dir)

    if not C.registered:
        print("åˆå§‹åŒ– Qlib...")
        qlib.init(
            provider_uri=qlib_init.get("provider_uri", "~/.qlib/qlib_data/cn_data"),
            region=REG_CN if qlib_init.get("region") == "cn" else None,
            exp_manager=qlib_init.get("exp_manager", {

                "class": "MLflowExpManager",
                "module_path": "qlib.workflow.expm",
                "kwargs": {
                    "uri": "file:" + str(Path(os.getcwd()).resolve() / "mlruns"),
                    "default_exp_name": "Experiment",
                },
            })
        )
    return cfg, work_dir


def train_model_alpha158(config_path=''):

    cfg, work_dir = init_qlib(config_path)

    # 3. åˆå§‹åŒ– MTSDatasetH
    dataset_cfg = cfg["task"]["dataset"]
    dataset = init_instance_by_config(dataset_cfg)

    '''

    # ç›´æ¥æ‹¿ handler
    handler = dataset.handler
    print(f"âœ… Handler åˆå§‹åŒ–å®Œæˆ: {type(handler)}")

    # 4. å¯¼å‡º feature å’Œ label
    feature_df = handler.fetch(col_set="feature")
    label_df = handler.fetch(col_set="label")

    print(f"Feature shape: {feature_df.shape}, Label shape: {label_df.shape}")

    # 5. ä¿å­˜ä¸º StaticDataLoader å¯ç”¨çš„æ–‡ä»¶
    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, "feature.pkl"), "wb") as f:
        pickle.dump(feature_df, f, protocol=pickle.HIGHEST_PROTOCOL)
    with open(os.path.join(output_dir, "label.pkl"), "wb") as f:
        pickle.dump(label_df, f, protocol=pickle.HIGHEST_PROTOCOL)

    print(f"âœ… å¯¼å‡ºå®Œæˆï¼š{output_dir}/feature.pkl, {output_dir}/label.pkl")

    # get features and labels
    # from qlib.data.dataset import DataHandlerLP
    # df_train, df_valid = dataset.prepare(["train", "valid"], col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
    # x_train, y_train = df_train["feature"], df_train["label"]
    # x_valid, y_valid = df_valid["feature"], df_valid["label"]

    '''

    port_analysis_config = cfg['port_analysis_config']
    # åˆå§‹åŒ– model
    model_cfg = cfg["task"]["model"]
    print(f"åˆå§‹åŒ– model: {model_cfg}")
    model = init_instance_by_config(model_cfg)

    ''' '''
    experiment_name = "workflow"
    recorder_info_file = f"{work_dir}/recorder_info_{datetime.now().strftime('%Y-%m-%d')}.json"
    # start exp
    train_model = None
    with R.start(experiment_name=experiment_name) as rec:
        print("å½“å‰ record_id:", rec.id)  # âœ… record_id å°±åœ¨è¿™é‡Œ
        # å½“å‰çš„å·¥ä½œ record
        active_recorder = rec.active_recorder
        # ä¿å­˜ info åˆ° JSON æ–‡ä»¶
        with open(recorder_info_file, "w", encoding="utf-8") as f:
            json.dump(active_recorder.info, f, indent=4, ensure_ascii=False)
        print(f"active_recorder_info: {json.dumps(active_recorder.info, indent=4)}" )

        # train model
        R.log_params(**flatten_dict(cfg["task"]))
        model.fit(dataset)
        print("model:", model)
        train_model = model

        recorder = R.get_recorder(recorder_id=active_recorder.id, experiment_name=experiment_name)
        recorder1 = R.get_recorder()
        # âœ… æ˜¾å¼ä¿å­˜æ¨¡å‹
        recorder.save_objects(model=model)

        # prediction
        # recorder = R.get_recorder()
        # é¢„æµ‹ + è¯„ä¼°
        sr = SignalRecord(model, dataset, recorder)
        sr.generate()

        # 2ï¸âƒ£ ä¿å­˜ IC / ICIR ç­‰æŒ‡æ ‡ï¼ˆå…³é”®ï¼‰
        SigAnaRecord(
            recorder=recorder,
            ana_long_short=False,
            ann_scaler=252,
        ).generate()

        # æµ‹è¯•
        '''
        PortAnaRecord(
            recorder=recorder,
            config=port_analysis_config,
        ).generate()
        '''

        # record = R.get_recorder(recorder_id="<record_id>")
        # model = record.load_object("model")

        # ä¿å­˜ info åˆ° JSON æ–‡ä»¶
        with open(recorder_info_file, "w", encoding="utf-8") as f:
            json.dump(active_recorder.info, f, indent=4, ensure_ascii=False)
        print(f"active_recorder_info: {json.dumps(active_recorder.info, indent=4)}")

    # åŠ è½½é˜¶æ®µ
    try:
        with open(recorder_info_file, "r", encoding="utf-8") as f:
            loaded_info_text = json.load(f)
        print("ä»æ–‡ä»¶åŠ è½½çš„ recorder_info å†…å®¹:")
        recorder_info_str = json.dumps(loaded_info_text, indent=4)
        recorder_info = json.loads(recorder_info_str)
        print( type(recorder_info) , recorder_info )
    except FileNotFoundError:
        print("æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œä¿å­˜éƒ¨åˆ†çš„ä»£ç ")

    # è·å– recorder,
    recorder = R.get_recorder(recorder_id=recorder_info['id'], experiment_id=recorder_info['experiment_id'])
    # record = get_recorder("workflow")
    model = recorder.load_object("model")
    print("model:", model)

def predict_data_model(config_path='', recorder_file=None):
    cfg, work_dir = init_qlib(config_path)
    if recorder_file:
        recorder_info_file = recorder_file
    else:
        recorder_info_file = f"{work_dir}/recorder_info_{datetime.now().strftime('%Y-%m-%d')}.json"
    # åŠ è½½é˜¶æ®µ
    try:
        with open(recorder_info_file, "r", encoding="utf-8") as f:
            loaded_info_text = json.load(f)
        print("ä»æ–‡ä»¶åŠ è½½çš„ recorder_info å†…å®¹:")
    except FileNotFoundError:
        print("æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œä¿å­˜éƒ¨åˆ†çš„ä»£ç ")

    # åŠ è½½ recorder_info
    recorder_info_str = json.dumps(loaded_info_text, indent=4)
    recorder_info = json.loads(recorder_info_str)
    print(type(recorder_info), recorder_info)

    # è·å– recorder, åŠ è½½æ¨¡å‹
    recorder = R.get_recorder(recorder_id=recorder_info['id'], experiment_id=recorder_info['experiment_id'])
    model = recorder.load_object("model")
    model._writer = None  # ğŸ”¥ å…³é”®, é¿å…_writeréœ€è¦åœ¨ train é˜¶æ®µåˆå§‹åŒ–ï¼Œç›´æ¥ predict å¤±è´¥
    print("model:", model)

    # åˆ›å»º dataset åˆå§‹åŒ– MTSDatasetH
    dataset_cfg = cfg["task"]["dataset"]
    print(f"åˆå§‹åŒ– MTSDatasetH, é…ç½®: {dataset_cfg}")

    dataset_cfg['kwargs']['handler']['kwargs']['start_time'] = date(2021, 1, 1)
    dataset_cfg['kwargs']['handler']['kwargs']['end_time'] = date(2025, 5, 15)

    dataset_cfg['kwargs']['segments'] = {
        'test': [date(2025, 1, 1), date(2025, 5, 15)]
    }

    print(f"ä¿®æ”¹å MTSDatasetH, é…ç½®: {dataset_cfg}")
    # å¯¹ dataset_cfg è¿›è¡Œä¿®æ”¹
    # 1. ä¿®æ”¹ end_time  'end_time': datetime.date(2025, 5, 15)
    #  ä¿æŒä¸å˜ fit_start_time: 2021-01-01  fit_end_time:   2024-12-31
    # 2.
    dataset = init_instance_by_config(dataset_cfg)

    print("dataset:", dataset)

    SignalRecord(
        model=model,
        dataset=dataset,
        recorder=recorder
    ).generate()

    pred = recorder.load_object("pred.pkl")

    print("pred type:", type(pred))
    print("pred:", pred)






if __name__ == "__main__":
    config_path = "./workflow_config_tra_Alpha158.yaml"
    # train_model_alpha158(config_path=config_path)

    predict_data_model(config_path=config_path)




