import datetime
import os
from pathlib import Path

import pandas as pd
import torch
import yaml
import importlib
import qlib
from jinja2 import Template, meta
from qlib.contrib.data.handler import Alpha158
from qlib.workflow import R, Experiment
from ruamel.yaml import YAML


def render_template(config_path: str,  render_env: dict={}) -> str:
    """
    render the template based on the environment or render_env

    Parameters
    ----------
    config_path : str
        configuration path

    Returns
    -------
    str
        the rendered content
    """
    with open(config_path, "r") as f:
        config = f.read()
    # Set up the Jinja2 environment
    template = Template(config)

    # Parse the template to find undeclared variables
    env = template.environment
    parsed_content = env.parse(config)
    variables = meta.find_undeclared_variables(parsed_content)

    # Get context from os.environ according to the variables
    context = {var: os.getenv(var, "") for var in variables if var in os.environ}
    context.update(render_env)
    print(f"Render the template with the context: {context}")

    # Render the template with the context
    rendered_content = template.render(context)
    return rendered_content


def load_config(config_path="config.yaml"):
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    :param config_path:
    :return:
    """
    # dataset_cls=TSDatasetH step_len=20 num_timesteps=20 PYTHONWARNINGS=ignore TF_CPP_MIN_LOG_LEVEL=2 PYTHONUNBUFFERED=1 qrun conf.yaml --experiment_name my_exp
    render_env = {"dataset_cls": 'TSDatasetH',
                  "num_timesteps": 20,
                  "step_len": 1}
    rendered_yaml = render_template(config_path, render_env=render_env)
    yaml = YAML(typ="safe", pure=True)
    config = yaml.load(rendered_yaml)
    return config


def import_object(obj_path, module_path):
    """æ”¯æŒæ¨¡å—è·¯å¾„å­—ç¬¦ä¸²å¯¼å…¥ç±»æˆ–å‡½æ•°"""
    if module_path:
        module = importlib.import_module(module_path)
        return getattr(module, obj_path)
    else:
        module_path, class_name = obj_path.rsplit(".", 1)
        module = importlib.import_module(module_path)
        return getattr(module, class_name)


def init_qlib(config, uri=None):
    mlflow_tracking_uri = f"{uri}/mlruns"
    os.environ["MLFLOW_TRACKING_URI"] = mlflow_tracking_uri
    os.environ["MLFLOW_DISABLE_ARTIFACT_CACHE"] = 'true'
    qlib.init(
        provider_uri=os.path.expanduser(config["qlib_init"]["provider_uri"]),
        region=config["qlib_init"].get("region", "cn")
    )
    # R.set_uri(uri=config["mlflow"]["uri"])
    # git_ignore_folder/RD-Agent_workspace/7b18b8c5cd83478db6fd1f8ad755522d/mlruns/153096223065276677/da7cb29b508149cb942e71c711556df5/artifacts/params.pkl
    # R.set_uri("/Users/dabai/Downloads/rd_agent_test/result_ck/7b18b8c5cd83478db6fd1f8ad755522d/mlruns")
    R.set_uri(mlflow_tracking_uri)


def get_latest_recorder(experiment_name):
    exp = R.get_exp(experiment_name=experiment_name)
    recorders = exp.list_recorders(rtype=Experiment.RT_L)
    if not recorders:
        raise ValueError(f"No recorders found in experiment '{experiment_name}'")
    return recorders[-1]


def build_dataset(config):
    handler_cfg = config['kwargs']["handler"]
    handler_cls = import_object(handler_cfg["class"], handler_cfg["module_path"])
    handler = handler_cls(**handler_cfg["kwargs"])

    dataset_cls = import_object(config["class"],  config["module_path"])
    return dataset_cls(
        handler=handler,
        segments=config['kwargs']["segments"]
    )


def build_model(config):
    model_cls = import_object(config["class"], config["module_path"])
    return model_cls(**config["kwargs"])


def predict_from_yaml(config_path="config.yaml", uri=None):
    config = load_config(config_path)

    # ä¿®æ”¹æ—¶é—´
    end_time = datetime.date(2025, 6, 20)
    config["task"]["dataset"]["kwargs"]["segments"]["test"] = [ datetime.date(2025, 6, 1),  end_time]
    config['data_handler_config']['end_time'] = end_time

    # config['data_handler_config']['label'] = ['Ref($close, -1) / $close - 1'] # ä¿®æ”¹ä¸ºäº†é¢„æµ‹æ˜å¤©çš„æ¶¨è·Œæ•°æ®ï¼Œä¸å¯ç”¨äºå›æµ‹ï¼Œåªä¸ºå¾—åˆ°æœ€åä¸€å¤©çš„é¢„æµ‹ç»“æœ

    init_qlib(config, uri= uri)

    '''
    data_handler_conf = config["data_handler_config"]
    # 3. æ„å»º handler å®ä¾‹
    handler = Alpha158(**data_handler_conf)
    # 4. è·å–æ¨ç†ç‰¹å¾æ•°æ®
    features = handler.fetch(col_set="feature")
    print("ç‰¹å¾æ•°æ®ç»´åº¦:", features.shape)
    # 5. æŸ¥çœ‹æœ€åå‡ å¤©çš„ç‰¹å¾æ˜¯å¦æœ‰ NaNï¼ˆæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè‚¡ç¥¨/æ—¶é—´ç»„åˆï¼‰
    print("\næœ€å10è¡Œç¼ºå¤±ç»Ÿè®¡ï¼š")
    print(features.tail(10).isna().sum(axis=1))
    # 6. è¿›ä¸€æ­¥ç­›æŸ¥ 2025-06-20 æ˜¯å¦å­˜åœ¨ï¼ˆç¡®è®¤æ¨ç†èŒƒå›´ï¼‰
    last_date = pd.to_datetime("2025-06-20")
    if last_date in features.index.get_level_values("datetime"):
        print(f"\nâœ… å­˜åœ¨ {last_date.date()} çš„ç‰¹å¾æ•°æ®")
    else:
        print(f"\nâŒ ä¸å­˜åœ¨ {last_date.date()} çš„ç‰¹å¾æ•°æ®")
    '''



    dataset = build_dataset(config['task']["dataset"])
    # model = build_model(config["task"]['model'])

    if "experiment_name" in config:
        experiment_name = config['experiment_name']
    else:
        experiment_name = 'workflow'
    recorder = get_latest_recorder(experiment_name)
    # æ—§è·¯å¾„
    # print("åŸå§‹ artifact_uri:", recorder.artifact_uri)
    # è®¾ç½®ä½ å¸Œæœ›çš„æ–°è·¯å¾„ï¼ˆæ³¨æ„åŠ  file:// å‰ç¼€ï¼‰
    # âœ… Monkey patch
    # recorder._artifact_uri = f"file://{recorder.uri}/{recorder.experiment_id}/{recorder.id}/artifacts"
    # print("æ–°çš„ artifact_uri:", recorder.artifact_uri)

    print(f"âœ… åŠ è½½æ¨¡å‹: params.pkl")
    model = recorder.load_object("params.pkl")

    model_save_dir = os.path.join(uri, "model_ckpt")
    if not os.path.exists(model_save_dir):
        os.makedirs(model_save_dir)
    best_model_path = os.path.join(model_save_dir, f"base_model_params.pt")
    torch.save(model.dnn_model, best_model_path)

    # trainset, validset, testset = dataset.prepare(["train", "valid", "test"])
    preds = model.predict(dataset, segment="test")
    print("ğŸ“Š é¢„æµ‹ç»“æœï¼ˆå‰å‡ è¡Œï¼‰ï¼š")
    print(preds.head())

    # 1. å°†ç´¢å¼•ä¸­çš„æ—¥æœŸæå–å‡ºæ¥ä½œä¸ºå•ç‹¬çš„åˆ—
    preds1 = preds.reset_index()

    # 2. ç­›é€‰æŒ‡å®šæ—¥æœŸï¼Œä¾‹å¦‚ï¼š2025-06-20
    date_str = '2025-06-20'
    specified_date = pd.to_datetime(date_str)  # ç¡®ä¿ä¸ç´¢å¼•çš„æ—¥æœŸç±»å‹ä¸€è‡´
    filtered_preds = preds1[preds1['datetime'] == specified_date]

    filtered_preds.columns = ['datetime', 'instrument', 'score']
    # ç­›é€‰ score å¤§äº 0 çš„æ•°æ®
    filtered_preds = filtered_preds[filtered_preds['score'] > 0]

    # æŒ‰ç…§ 'score' åˆ—é™åºæ’åˆ—
    sorted_preds = filtered_preds.sort_values(by='score', ascending=False)

    # ä¿å­˜ä¸º CSV æ–‡ä»¶
    result_path = os.path.join(Path(uri).parent, "result")
    if not os.path.exists(result_path):
        os.makedirs(result_path)
    output_file = f"{result_path}/{date_str}_{Path(uri).name}_sorted_preds.csv"
    sorted_preds.to_csv(output_file, index=False)

    # 4. å–å‡ºå‰20æ¡æ•°æ®
    top_preds = sorted_preds.head(30)

    print(f"ğŸ“Š ç­›é€‰å¹¶æ’åºåçš„å‰30æ¡æ•°æ®ï¼ˆ{specified_date}ï¼‰:")
    print(top_preds)

    print('\n\n\n\n\n')


if __name__ == "__main__":

    # OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 PYTHONFAULTHANDLER=1
    os.environ["OMP_NUM_THREADS"] = '1'
    os.environ["MKL_NUM_THREADS"] = '1'
    os.environ["PYTHONFAULTHANDLER"] = '1'
    # PYTHONWARNINGS=ignore TF_CPP_MIN_LOG_LEVEL=2 PYTHONUNBUFFERED=1
    os.environ["PYTHONWARNINGS"] = 'ignore'
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = '2'
    os.environ["PYTHONUNBUFFERED"] = '1'

    base_path = '/Users/dabai/work/data/agent_qlib/online_check/fin_model_orig/2025-06-07_07-14-29-272373/csi300_qwen30b_loop36' # mlruns/419691347954910967/514f5ce587374c03a6f1b83f524f9fae

    path_list = [
        '/Users/dabai/work/data/agent_qlib/online_check/fin_model_orig/2025-06-07_07-14-29-272373/csi300_qwen30b_loop36',
        '/Users/dabai/work/data/agent_qlib/online_check/fin_model_orig/2025-06-07_07-14-29-272373/csi300_qwen30b_loop19',
        '/Users/dabai/work/data/agent_qlib/online_check/fin_model_orig/2025-06-07_07-14-29-272373/csi300_qwen30b_loop21',
        '/Users/dabai/work/data/agent_qlib/online_check/fin_model_orig/2025-06-07_07-14-29-272373/csi300_qwen30b_loop44',
        '/Users/dabai/work/data/agent_qlib/online_check/fin_model_orig/2025-06-07_07-14-29-272373/csi300_qwen30b_loop61',

    ]

    # for i, base_path in enumerate(path_list):
    #
    #     import sys
    #     print(i, sys.path)
    #     if i>0:
    #         sys.path.remove(path_list[i-1])
    #     sys.path.append(base_path)
    #     print(i, sys.path)
    #
    #     # å¦‚æœæ¨¡å—å·²ç»å¯¼å…¥ï¼Œéœ€è¦åˆ é™¤ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡é‡æ–°åŠ è½½
    #     module_name = "model"  # æ›¿æ¢ä¸ºä½ è¦é‡æ–°åŠ è½½çš„æ¨¡å—å
    #     if module_name in sys.modules:
    #         del sys.modules[module_name]
    #
    #     config_path = f'{base_path}/conf.yaml'
    #     # config_path = 'config.yaml'
    #
    #     predict_from_yaml(config_path, uri=base_path)


    df_list = []
    for base_path in path_list:
        print(base_path)
        date_str = '2025-06-20'
        result_path = os.path.join(Path(base_path).parent, "result")
        output_file = f"{result_path}/{date_str}_{Path(base_path).name}_sorted_preds.csv"

        df = pd.read_csv(output_file)
        df_list.append(df)

    # base_p = '/Users/dabai/liepin/é£ä¹¦_bak/æœç´¢ç®—æ³•/æœç´¢å¬å›/result/'
    # df_list = []
    # for p_ in os.listdir(base_p):
    #     if p_.endswith('csv'):
    #         df = pd.read_csv(base_p + p_)
    #         df_list.append(df)

    # æ­¥éª¤ 1ï¼šåˆå¹¶æ‰€æœ‰æ•°æ®
    all_df = pd.concat(df_list, ignore_index=True)

    # æ­¥éª¤ 2ï¼šæŒ‰ instrument èšåˆï¼Œç»Ÿè®¡æ¬¡æ•°å’Œ score å¹³å‡
    agg_df = all_df.groupby(["datetime", "instrument"]).agg(
        count=("score", "count"),
        avg_score=("score", "mean"),
        sum_score=("score", "sum")
    ).reset_index()

    # æ­¥éª¤ 3ï¼šæ’åº - å…ˆæŒ‰ count é™åºï¼Œå†æŒ‰ avg_score é™åº
    agg_df = agg_df.sort_values(by=["avg_score", "count" ], ascending=[False, False])

    # è¾“å‡ºå‰å‡ è¡Œç»“æœ
    print(agg_df.head())

    config_path = f'{base_path}/conf.yaml'
    config = load_config(config_path)

    output_file = f"{result_path}/merge_sorted_preds.csv"
    agg_df.to_csv(output_file, index=False)

    init_qlib(config, uri=base_path)
    # dataset = build_dataset(config['task']["dataset"])

    # è·å–æŒ‡å®šå¸‚åœºæ•°æ®ï¼Œä½¿ç”¨ instruments æŒ‡å®šå¸‚åœºï¼ˆå¦‚ csi300ï¼‰
    market = 'csi300'  # å¯ä»¥æ¢æˆå…¶ä»–å¸‚åœºåï¼Œæ¯”å¦‚ 'szse', 'hs300' ç­‰
    from qlib.data import D

    start_time = '2025-06-20'
    end_time = '2025-06-20'
    # è·å–æŸä¸ªæ—¥æœŸçš„æŒ‡å®šå¸‚åœºè‚¡ç¥¨æ•°æ®
    instruments = D.instruments(market=market)
    instrument_list = D.list_instruments(instruments=instruments, start_time=start_time, end_time=end_time, as_list=True)
    required_fields = ["$open", "$close", "$low", "$high", "$volume", "$factor", "Ref($close, -1) / $close - 1"]

    df = D.features(instrument_list, required_fields,  start_time=start_time,  end_time=end_time)
    df.rename(
        columns={
            "$open": "open",
            "$close": "close",
            "$low": "low",
            "$high": "high",
            "$volume": "volume",
            "$factor": "factor",
            "Ref($close, -1) / $close - 1": "lab",
        },
        inplace=True,
    )
    df = df.reset_index()
    print(df)
    df['datetime'] = pd.to_datetime(df['datetime']).dt.strftime("%Y-%m-%d")
    result = pd.merge(df, agg_df, on=['datetime', 'instrument'],  how='inner')
    output_file = f"{result_path}/merge_lab_preds.csv"
    result.to_csv(output_file, index=False)
    print('vgafdg')



    # import joblib
    #
    # params_path = '/Users/dabai/Downloads/rd_agent_test/git_ignore_folder/RD-Agent_workspace/7b18b8c5cd83478db6fd1f8ad755522d/mlruns/153096223065276677/da7cb29b508149cb942e71c711556df5/artifacts/params.pkl'
    # params_path = '/Users/dabai/Downloads/rd_agent_test/git_ignore_folder/RD-Agent_workspace/7b18b8c5cd83478db6fd1f8ad755522d/ret.pkl'
    # params = joblib.load(params_path)
    # print(params)


# model: Model = init_instance_by_config(task_config["model"], accept_types=Model)
# dataset: Dataset = init_instance_by_config(task_config["dataset"], accept_types=Dataset)